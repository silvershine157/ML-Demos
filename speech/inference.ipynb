{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import *\n",
    "from dataset import *\n",
    "from const import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from singlefit import *\n",
    "def single_inference():\n",
    "    net = SingleDecoder()\n",
    "    net.load_state_dict(torch.load('data/ckpts/single_latest.sd'))\n",
    "    ds, loader = get_lj_loader(batch_size=1, limit=1, get_dataset=True)\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            S_pad, S_lengths, token_pad, token_lengths = batch\n",
    "            S_pad = S_pad.to(device)\n",
    "            S_pred = net(S_pad)\n",
    "\n",
    "    wav, _, _, _ = ds.base_ds[0]\n",
    "    \n",
    "    S_base_ds = ds.wav_to_spec(wav)\n",
    "    S_ds, _ = ds[0]\n",
    "    S_loaded = S_pad[:, 0, :].cpu()\n",
    "    \n",
    "    #recon_wav = ds.spec_to_wav(S_base_ds)\n",
    "    #recon_wav = ds.spec_to_wav(S_ds)\n",
    "    recon_wav = ds.spec_to_wav(S_loaded)\n",
    "    sr = 22050\n",
    "    librosa.output.write_wav('data/recon_sample.wav', recon_wav.squeeze().numpy(), sr)\n",
    "    \n",
    "single_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_reconstruct_demo():\n",
    "    sr = 22050\n",
    "    ds = LinSpecDataset(torchaudio.datasets.LJSPEECH('./data'), limit=1)\n",
    "    orig_wav, _, _, _ = ds.base_ds[0]\n",
    "    orig_S = ds.wav_to_spec(orig_wav)\n",
    "    print(orig_S.shape)\n",
    "    recon_wav = ds.spec_to_wav(orig_S)\n",
    "    librosa.output.write_wav('data/recon_sample.wav', recon_wav.squeeze().numpy(), sr)\n",
    "torch_reconstruct_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio('data/orig_sample.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio('data/recon_sample.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
